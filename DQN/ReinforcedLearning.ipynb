{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# 학습에 필요한 설정값들을 선언합니다.\n",
    "epsilon = 1                         # epsilon-Greedy 기법에 사용할 최초의 epsilon값\n",
    "epsilonMinimumValue = 0.001         # epsilon의 최소값 (이 값 이하로 Decay하지 않습니다)\n",
    "num_actions = 3                     # 에이전트가 취할 수 있는 행동의 개수 - (좌로 움직이기, 가만히 있기, 우로 움직이기)\n",
    "num_epochs = 2000                   # 학습에 사용할 반복횟수\n",
    "hidden_size = 128                   # 히든레이어의 노드 개수\n",
    "maxMemory = 500                     # Replay Memory의 크기\n",
    "batch_size = 50                     # 학습에 사용할 배치 개수\n",
    "gridSize = 10                       # 에이전트가 플레이하는 게임 화면 크기 (10x10 grid)\n",
    "state_size = gridSize * gridSize    # 게임 환경의 현재상태 (10x10 grid)\n",
    "discount = 0.9                      # Discount Factor \\gamma\n",
    "learning_rate = 0.2                 # 러닝 레이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randf(s, e):\n",
    "  return (float(random.randrange(0, (e - s) * 9999)) / 10000) + s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "  # DQN 모델을 위한 tf.Variable들을 정의합니다.\n",
    "  def __init__(self):\n",
    "    # 100(현재 상태 - 10x10 Grid) -> 128 -> 128 -> 3(예측된 각 행동의 Q값)\n",
    "    self.W1 = tf.Variable(tf.random.truncated_normal(shape=[state_size, hidden_size], stddev=1.0 / math.sqrt(float(state_size))))\n",
    "    self.b1 = tf.Variable(tf.random.truncated_normal(shape=[hidden_size], stddev=0.01))\n",
    "    self.W2 = tf.Variable(tf.random.truncated_normal(shape=[hidden_size, hidden_size],stddev=1.0 / math.sqrt(float(hidden_size))))\n",
    "    self.b2 = tf.Variable(tf.random.truncated_normal(shape=[hidden_size], stddev=0.01))\n",
    "    self.W3 = tf.Variable(tf.random.truncated_normal(shape=[hidden_size, num_actions],stddev=1.0 / math.sqrt(float(hidden_size))))\n",
    "    self.b3 = tf.Variable(tf.random.truncated_normal(shape=[num_actions], stddev=0.01))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    H1_output = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n",
    "    H2_output = tf.nn.relu(tf.matmul(H1_output, self.W2) + self.b2)\n",
    "    output_layer = tf.matmul(H2_output, self.W3) + self.b3\n",
    "\n",
    "    return tf.squeeze(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y):\n",
    "  return tf.reduce_sum(tf.square(y-y_pred)) / (2*batch_size)  # MSE 손실 함수\n",
    "\n",
    "# 옵티마이저를 정의합니다.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화를 위한 function을 정의합니다.\n",
    "def train_step(model, x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_pred = model(x)\n",
    "    loss = mse_loss(y_pred, y)\n",
    "  gradients = tape.gradient(loss, vars(model).values())\n",
    "  optimizer.apply_gradients(zip(gradients, vars(model).values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatchGame을 수행하는 Environment를 구현합니다.\n",
    "class CatchEnvironment():\n",
    "  # 상태의 초기값을 지정합니다.\n",
    "  def __init__(self, gridSize):\n",
    "    self.gridSize = gridSize\n",
    "    self.state_size = self.gridSize * self.gridSize\n",
    "    self.state = np.empty(3, dtype = np.uint8) \n",
    "\n",
    "  # 관찰 결과를 리턴합니다.\n",
    "  def observe(self):\n",
    "    canvas = self.drawState()\n",
    "    canvas = np.reshape(canvas, (-1,self.state_size))\n",
    "    return canvas.astype('float32')\n",
    "\n",
    "  # 현재 상태(fruit, basket)를 화면에 출력합니다.\n",
    "  def drawState(self):\n",
    "    canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "    # fruit를 화면에 그립니다.\n",
    "    canvas[self.state[0]-1, self.state[1]-1] = 1  \n",
    "    # basket을 화면에 그립니다. \n",
    "    canvas[self.gridSize-1, self.state[2] -1 - 1] = 1\n",
    "    canvas[self.gridSize-1, self.state[2] -1] = 1\n",
    "    canvas[self.gridSize-1, self.state[2] -1 + 1] = 1    \n",
    "    return canvas        \n",
    "\n",
    "  # 게임을 초기 상태로 리셋합니다.\n",
    "  def reset(self): \n",
    "    initialFruitColumn = random.randrange(1, self.gridSize + 1)\n",
    "    initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)\n",
    "    self.state = np.array([1, initialFruitColumn, initialBucketPosition]) \n",
    "    return self.getState()\n",
    "\n",
    "  # 현재 상태를 불러옵니다.\n",
    "  def getState(self):\n",
    "    stateInfo = self.state\n",
    "    fruit_row = stateInfo[0]\n",
    "    fruit_col = stateInfo[1]\n",
    "    basket = stateInfo[2]\n",
    "    return fruit_row, fruit_col, basket\n",
    "\n",
    "  # 에이전트가 취한 행동에 대한 보상을 줍니다.\n",
    "  def getReward(self):\n",
    "    fruitRow, fruitColumn, basket = self.getState()\n",
    "    # 만약 fruit가 바닥에 닿았을 때\n",
    "    if (fruitRow == self.gridSize - 1):  \n",
    "      # basket이 fruit을 받아내면 1의 reward를 줍니다.\n",
    "      if (abs(fruitColumn - basket) <= 1): \n",
    "        return 1\n",
    "      # fruit를 받아내지 못하면 -1의 reward를 줍니다.\n",
    "      else:\n",
    "        return -1\n",
    "    # fruit가 바닥에 닿지 않은 중립적인 상태는 0의 reward를 줍니다.\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  # 게임이 끝났는지를 체크합니다.(fruit가 바닥에 닿으면 한게임이 종료됩니다.)\n",
    "  def isGameOver(self):\n",
    "    if (self.state[0] == self.gridSize - 1): \n",
    "      return True \n",
    "    else: \n",
    "      return False \n",
    "\n",
    "  # action(좌로 한칸 이동, 제자리, 우로 한칸이동)에 따라 basket의 위치를 업데이트합니다.\n",
    "  def updateState(self, action):\n",
    "    move = 0\n",
    "    if (action == 0):\n",
    "      move = -1\n",
    "    elif (action == 1):\n",
    "      move = 0\n",
    "    elif (action == 2):\n",
    "      move = 1\n",
    "    fruitRow, fruitColumn, basket = self.getState()\n",
    "    newBasket = min(max(2, basket + move), self.gridSize - 1) # min/max는 basket이 grid밖으로 벗어나는것을 방지합니다.\n",
    "    fruitRow = fruitRow + 1  # fruit는 매 행동을 취할때마다 1칸씩 아래로 떨어집니다. \n",
    "    self.state = np.array([fruitRow, fruitColumn, newBasket])\n",
    "\n",
    "  # 행동을 취합니다. 0 : 왼쪽으로 이동, 1 : 가만히 있기, 2 : 오른쪽으로 이동\n",
    "  def act(self, action):\n",
    "    self.updateState(action)\n",
    "    reward = self.getReward()\n",
    "    gameOver = self.isGameOver()\n",
    "    return self.observe(), reward, gameOver, self.getState()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "  def __init__(self, gridSize, maxMemory, discount):\n",
    "    self.maxMemory = maxMemory\n",
    "    self.gridSize = gridSize\n",
    "    self.state_size = self.gridSize * self.gridSize\n",
    "    self.discount = discount\n",
    "    self.inputState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "    self.actions = np.zeros(self.maxMemory, dtype = np.uint8)\n",
    "    self.nextState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "    self.gameOver = np.empty(self.maxMemory, dtype = np.bool)\n",
    "    self.rewards = np.empty(self.maxMemory, dtype = np.int8) \n",
    "    self.count = 0\n",
    "    self.current = 0\n",
    "\n",
    "  # 경험을 Replay Memory에 저장합니다.\n",
    "  def remember(self, currentState, action, reward, nextState, gameOver):\n",
    "    self.actions[self.current] = action\n",
    "    self.rewards[self.current] = reward\n",
    "    self.inputState[self.current, ...] = currentState\n",
    "    self.nextState[self.current, ...] = nextState\n",
    "    self.gameOver[self.current] = gameOver\n",
    "    self.count = max(self.count, self.current + 1)\n",
    "    self.current = (self.current + 1) % self.maxMemory\n",
    "\n",
    "  def getBatch(self, DQN_model, batch_size, num_actions, state_size):\n",
    "    # 취할 수 있는 가장 큰 배치 사이즈를 선택합니다. (학습 초기에는 batch_size만큼의 기억이 없습니다.)\n",
    "    memoryLength = self.count\n",
    "    chosenBatchSize = min(batch_size, memoryLength)\n",
    "\n",
    "    # 인풋 데이터와 타겟데이터를 선언합니다. \n",
    "    inputs = np.zeros((chosenBatchSize, state_size))\n",
    "    targets = np.zeros((chosenBatchSize, num_actions))\n",
    "\n",
    "    # 배치안의 값을 설정합니다.\n",
    "    for i in range(chosenBatchSize):\n",
    "      # 배치에 포함될 기억을 랜덤으로 선택합니다.\n",
    "      randomIndex = random.randrange(0, memoryLength)\n",
    "      # 현재 상태와 Q값을 불러옵니다.\n",
    "      current_inputState = np.reshape(self.inputState[randomIndex], (1, 100))\n",
    "      target = DQN_model(current_inputState).numpy()\n",
    "\n",
    "      # 현재 상태 바로 다음 상태를 불러오고 다음 상태에서 취할수 있는 가장 큰 Q값을 계산합니다.\n",
    "      current_nextState = np.reshape(self.nextState[randomIndex], (1, 100))\n",
    "      nextStateQ = DQN_model(current_nextState).numpy()\n",
    "      nextStateMaxQ = np.amax(nextStateQ)\n",
    "      # 만약 게임오버라면 reward로 Q값을 업데이트하고 \n",
    "      if (self.gameOver[randomIndex] == True):\n",
    "        target[self.actions[randomIndex]] = self.rewards[randomIndex]\n",
    "      # 게임오버가 아니라면 타겟 Q값(최적의 Q값)을 아래 수식을 이용해서 계산합니다.\n",
    "      # Q* = reward + discount(gamma) * max_a' Q(s',a')\n",
    "      else:\n",
    "        target[self.actions[randomIndex]] = self.rewards[randomIndex] + self.discount * nextStateMaxQ\n",
    "\n",
    "      # 인풋과 타겟 데이터에 값을 지정합니다.\n",
    "      inputs[i] = current_inputState\n",
    "      targets[i] = target\n",
    "\n",
    "    return inputs.astype('float32'), targets.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  print(\"트레이닝을 시작합니다.\")\n",
    "\n",
    "  # 게임 플레이 환경을 선언합니다.\n",
    "  env = CatchEnvironment(gridSize)\n",
    "\n",
    "  # Replay Memory를 선언합니다.\n",
    "  memory = ReplayMemory(gridSize, maxMemory, discount)\n",
    "\n",
    "  # 학습된 파라미터를 저장하기 위한 tf.train.CheckpointManager를 선언합니다.\n",
    "  ckpt = tf.train.Checkpoint(**vars(DQN_model))\n",
    "  ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, directory=os.getcwd(), max_to_keep=5, checkpoint_name='model.ckpt')\n",
    "\n",
    "  winCount = 0\n",
    "\n",
    "  for i in range(num_epochs+1):\n",
    "    # 환경을 초기화합니다.\n",
    "    err = 0\n",
    "    env.reset()\n",
    "\n",
    "    isGameOver = False\n",
    "\n",
    "    # 최초의 상태를 불러옵니다.\n",
    "    currentState = env.observe()\n",
    "\n",
    "    while (isGameOver != True):\n",
    "      action = -9999  # Q값을 초기화합니다.\n",
    "      # epsilon-Greedy 기법에 따라 랜덤한 행동을 할지 최적의 행동을 할지를 결정합니다.\n",
    "      global epsilon\n",
    "      if (randf(0, 1) <= epsilon):\n",
    "        # epsilon 확률만큼 랜덤한 행동을 합니다.\n",
    "        action = random.randrange(0, num_actions)\n",
    "      else:\n",
    "        # (1-epsilon) 확률만큼 최적의 행동을 합니다.\n",
    "        # 현재 상태를 DQN의 인풋으로 넣어서 예측된 최적의 Q(s,a)값들을 리턴받습니다.\n",
    "        q = DQN_model(currentState).numpy()\n",
    "        # Q(s,a)가 가장 높은 행동을 선택합니다.\n",
    "        action = q.argmax()\n",
    "\n",
    "      # epsilon값을 0.9999만큼 Decay합니다.\n",
    "      if (epsilon > epsilonMinimumValue):\n",
    "        epsilon = epsilon * 0.999\n",
    "\n",
    "      # 에이전트가 행동을 하고 다음 보상과 다음 상태에 대한 정보를 리턴 받습니다.\n",
    "      nextState, reward, gameOver, stateInfo = env.act(action)\n",
    "\n",
    "      # 만약 과일을 제대로 받아냈다면 승리 횟수를 1 올립니다.\n",
    "      if (reward == 1):\n",
    "        winCount = winCount + 1\n",
    "\n",
    "      # 에이전트가 수집한 정보를 Replay Memory에 저장합니다.\n",
    "      memory.remember(currentState, action, reward, nextState, gameOver)\n",
    "\n",
    "      # 현재 상태를 다음 상태로 업데이트하고 GameOver유무를 체크합니다.\n",
    "      currentState = nextState\n",
    "      isGameOver = gameOver\n",
    "\n",
    "      # Replay Memory로부터 학습에 사용할 Batch 데이터를 불러옵니다.\n",
    "      inputs, targets = memory.getBatch(DQN_model, batch_size, num_actions, state_size)\n",
    "\n",
    "      # 최적화를 수행하고 손실함수를 리턴받습니다.\n",
    "      _, loss_print = train_step(DQN_model, inputs, targets), mse_loss(DQN_model(inputs), targets)\n",
    "      err = err + loss_print\n",
    "\n",
    "    print(\"반복(Epoch): %d, 에러(err): %.4f, 승리횟수(Win count): %d, 승리비율(Win ratio): %.4f\" % (i, err, winCount, float(winCount)/float(i+1)*100))\n",
    "  # 학습이 모두 끝나면 파라미터를 지정된 경로에 저장합니다.\n",
    "  print(\"트레이닝 완료\")\n",
    "  save_path = ckpt_manager.save(checkpoint_number=i)\n",
    "  print(\"%s 경로에 파라미터가 저장되었습니다\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1029369925491739755\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12179687463126033037\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14196069548382443251\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레이닝을 시작합니다.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "/job:localhost/replica:0/task:0/device:GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-be0fb26d8432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-2c44f9588c6f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# (1-epsilon) 확률만큼 최적의 행동을 합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 현재 상태를 DQN의 인풋으로 넣어서 예측된 최적의 Q(s,a)값들을 리턴받습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Q(s,a)가 가장 높은 행동을 선택합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3e8734bb340b>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mH1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mH2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH2_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_resource_variable_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_resource_variable_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/morpheus/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /job:localhost/replica:0/task:0/device:GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    main(__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
